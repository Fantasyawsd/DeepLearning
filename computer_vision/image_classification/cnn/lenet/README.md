# LeNet-5 æ¨¡å‹ä½¿ç”¨æŒ‡å—

LeNet-5 æ˜¯ç”± Yann LeCun åœ¨1998å¹´æå‡ºçš„ç»å…¸å·ç§¯ç¥ç»ç½‘ç»œï¼Œæ˜¯æ·±åº¦å­¦ä¹ å†å²ä¸Šçš„é‡è¦é‡Œç¨‹ç¢‘ã€‚æœ¬æŒ‡å—è¯¦ç»†ä»‹ç» LeNet-5 çš„ä½¿ç”¨æ–¹æ³•ã€‚

## ğŸ“– æ¨¡å‹ç®€ä»‹

LeNet-5 æ˜¯æœ€æ—©çš„æˆåŠŸçš„å·ç§¯ç¥ç»ç½‘ç»œä¹‹ä¸€ï¼Œä¸»è¦ç‰¹ç‚¹ï¼š

- **å†å²æ„ä¹‰**: æ·±åº¦å­¦ä¹ çš„å¼€åˆ›æ€§å·¥ä½œ
- **ç»“æ„ç®€å•**: 2ä¸ªå·ç§¯å±‚ + 3ä¸ªå…¨è¿æ¥å±‚
- **è½»é‡çº§**: ä»…çº¦60Kå‚æ•°
- **ç»å…¸åº”ç”¨**: æ‰‹å†™æ•°å­—è¯†åˆ«ã€é‚®æ”¿ç¼–ç è¯†åˆ«

### ç½‘ç»œæ¶æ„

```
è¾“å…¥(32Ã—32) â†’ Conv1(6@28Ã—28) â†’ Pool1(6@14Ã—14) â†’ Conv2(16@10Ã—10) â†’ Pool2(16@5Ã—5) â†’ FC1(120) â†’ FC2(84) â†’ FC3(10)
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬ä½¿ç”¨

```python
import torch
from model import LeNet5

# åˆ›å»ºæ¨¡å‹ (MNISTé…ç½®)
config = {
    'num_classes': 10,
    'input_channels': 1,
    'input_size': 32
}
model = LeNet5(config)

# å‰å‘ä¼ æ’­
x = torch.randn(1, 1, 32, 32)  # æ‰¹æ¬¡å¤§å°1ï¼Œ1é€šé“ï¼Œ32Ã—32å›¾åƒ
output = model(x)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # [1, 10]
```

### 2. ä½¿ç”¨é…ç½®æ–‡ä»¶è®­ç»ƒ

```bash
# è®­ç»ƒMNISTæ•°æ®é›†
python train.py --config config.yaml

# ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
python train.py --config config.yaml --resume checkpoints/lenet/checkpoint_latest.pth
```

### 3. æ¨¡å‹æµ‹è¯•

```bash
# æµ‹è¯•æ¨¡å‹æ€§èƒ½
python test.py --config config.yaml --checkpoint checkpoints/lenet/checkpoint_best.pth

# å¯è§†åŒ–é¢„æµ‹ç»“æœ
python test.py --config config.yaml --checkpoint checkpoints/lenet/checkpoint_best.pth --visualize
```

## âš™ï¸ é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½®

```yaml
model:
  num_classes: 10        # åˆ†ç±»ç±»åˆ«æ•°
  input_channels: 1      # è¾“å…¥é€šé“æ•° (ç°åº¦å›¾:1, RGBå›¾:3)
  input_size: 32         # è¾“å…¥å›¾åƒå°ºå¯¸
  dropout: 0.0           # Dropoutæ¯”ä¾‹ (åŸå§‹LeNetä¸ä½¿ç”¨)
```

### æ•°æ®é›†æ”¯æŒ

- **MNIST**: æ‰‹å†™æ•°å­—è¯†åˆ« (28Ã—28 â†’ 32Ã—32)
- **CIFAR-10**: 10ç±»è‡ªç„¶å›¾åƒ (32Ã—32)
- **CIFAR-100**: 100ç±»è‡ªç„¶å›¾åƒ (32Ã—32)

### è®­ç»ƒé…ç½®

```yaml
training:
  batch_size: 128
  epochs: 50
  learning_rate: 0.001
  weight_decay: 1e-4
  
  optimizer:
    type: "Adam"         # Adam, SGD, AdamW
    momentum: 0.9        # SGDä½¿ç”¨
  
  scheduler:
    type: "StepLR"       # StepLR, CosineAnnealingLR
    step_size: 15
    gamma: 0.1
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

### MNISTæ•°æ®é›†

| é…ç½® | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´ | æµ‹è¯•ç²¾åº¦ |
|------|--------|----------|----------|
| æ ‡å‡†LeNet-5 | 60K | ~5åˆ†é’Ÿ | 99%+ |
| æ”¹è¿›ç‰ˆ(BatchNorm) | 60K | ~5åˆ†é’Ÿ | 99.3%+ |

### CIFAR-10æ•°æ®é›†

| é…ç½® | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´ | æµ‹è¯•ç²¾åº¦ |
|------|--------|----------|----------|
| LeNet-5(RGB) | 62K | ~10åˆ†é’Ÿ | 65-70% |
| LeNet-5+æ•°æ®å¢å¼º | 62K | ~12åˆ†é’Ÿ | 70-75% |

## ğŸ”§ é«˜çº§ç”¨æ³•

### 1. è‡ªå®šä¹‰æ¨¡å‹

```python
from model import LeNet5

# åˆ›å»ºè‡ªå®šä¹‰é…ç½®
config = {
    'num_classes': 100,    # CIFAR-100
    'input_channels': 3,   # RGBå›¾åƒ
    'input_size': 32,
    'dropout': 0.2         # æ·»åŠ Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
}

model = LeNet5(config)
print(f"æ¨¡å‹å‚æ•°é‡: {model.count_parameters():,}")
```

### 2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

```python
from load_pretrained import load_pretrained_lenet

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = load_pretrained_lenet('lenet5_mnist')

# ä»æ£€æŸ¥ç‚¹åŠ è½½
from load_pretrained import create_lenet_from_checkpoint
model = create_lenet_from_checkpoint('checkpoints/lenet/checkpoint_best.pth')
```

### 3. æ¨¡å‹æ¨ç†

```python
from test import LeNetTester

# åˆ›å»ºæµ‹è¯•å™¨
tester = LeNetTester('config.yaml', 'checkpoints/lenet/checkpoint_best.pth')

# å•å¼ å›¾åƒæ¨ç†
result = tester.predict_single(image_tensor)
print(f"é¢„æµ‹ç±»åˆ«: {result['predicted_class_name']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']:.2f}")
```

### 4. ç‰¹å¾å¯è§†åŒ–

```python
import torch
import matplotlib.pyplot as plt

# è·å–ç¬¬ä¸€ä¸ªå·ç§¯å±‚çš„æƒé‡
conv1_weights = model.conv1.weight.data

# å¯è§†åŒ–å·ç§¯æ ¸
fig, axes = plt.subplots(2, 3, figsize=(9, 6))
for i, ax in enumerate(axes.flat):
    if i < conv1_weights.shape[0]:
        # æ˜¾ç¤ºç¬¬iä¸ªå·ç§¯æ ¸
        kernel = conv1_weights[i, 0]  # ç¬¬0ä¸ªè¾“å…¥é€šé“
        ax.imshow(kernel, cmap='gray')
        ax.set_title(f'å·ç§¯æ ¸ {i+1}')
        ax.axis('off')

plt.tight_layout()
plt.show()
```

## ğŸ¯ è®­ç»ƒæŠ€å·§

### 1. æ•°æ®é¢„å¤„ç†

```python
# MNISTæ•°æ®é¢„å¤„ç†
transforms.Compose([
    transforms.Resize(32),      # è°ƒæ•´åˆ°LeNetè¾“å…¥å°ºå¯¸
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))  # MNISTæ ‡å‡†åŒ–
])

# CIFARæ•°æ®é¢„å¤„ç†
transforms.Compose([
    transforms.RandomHorizontalFlip(),  # æ•°æ®å¢å¼º
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), 
                        (0.2023, 0.1994, 0.2010))
])
```

### 2. å­¦ä¹ ç‡è°ƒåº¦

```python
# é˜¶æ¢¯è¡°å‡
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)

# ä½™å¼¦é€€ç«
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)

# è‡ªé€‚åº”è°ƒæ•´
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5)
```

### 3. é˜²æ­¢è¿‡æ‹Ÿåˆ

```python
# 1. æ·»åŠ Dropout
config['dropout'] = 0.2

# 2. æ•°æ®å¢å¼º
data_config = {
    'augmentation': {
        'enabled': True,
        'horizontal_flip': True,
        'rotation': 10
    }
}

# 3. æ­£åˆ™åŒ–
optimizer = optim.Adam(model.parameters(), weight_decay=1e-4)
```

## ğŸ“ˆ æ¨¡å‹åˆ†æ

### 1. å‚æ•°ç»Ÿè®¡

```python
# æ€»å‚æ•°é‡
total_params = model.count_parameters()
print(f"æ€»å‚æ•°é‡: {total_params:,}")

# å„å±‚å‚æ•°é‡
for name, param in model.named_parameters():
    print(f"{name}: {param.numel():,} å‚æ•°")
```

### 2. è®¡ç®—å¤æ‚åº¦

```python
from torchprofile import profile_macs

# è®¡ç®—FLOPs
input_tensor = torch.randn(1, 1, 32, 32)
macs = profile_macs(model, input_tensor)
print(f"FLOPs: {macs:,}")
```

### 3. æ¨ç†é€Ÿåº¦

```python
import time

model.eval()
input_tensor = torch.randn(100, 1, 32, 32)

# é¢„çƒ­
for _ in range(10):
    _ = model(input_tensor)

# æµ‹è¯•æ¨ç†é€Ÿåº¦
start_time = time.time()
for _ in range(100):
    with torch.no_grad():
        _ = model(input_tensor)
end_time = time.time()

avg_time = (end_time - start_time) / 100 / 100  # æ¯å¼ å›¾åƒæ—¶é—´
print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time*1000:.2f}ms/å›¾åƒ")
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **è¾“å…¥å°ºå¯¸é”™è¯¯**
   ```python
   # ç¡®ä¿è¾“å…¥å°ºå¯¸ä¸º32x32
   assert input.shape[-2:] == (32, 32), f"è¾“å…¥å°ºå¯¸åº”ä¸º32x32ï¼Œå½“å‰ä¸º{input.shape[-2:]}"
   ```

2. **ç±»åˆ«æ•°ä¸åŒ¹é…**
   ```python
   # æ£€æŸ¥é…ç½®ä¸­çš„ç±»åˆ«æ•°
   assert config['num_classes'] == len(class_names), "ç±»åˆ«æ•°ä¸æ ‡ç­¾æ•°ä¸åŒ¹é…"
   ```

3. **GPUå†…å­˜ä¸è¶³**
   ```python
   # å‡å°æ‰¹æ¬¡å¤§å°
   config['training']['batch_size'] = 64  # ä»128å‡å°åˆ°64
   ```

### è°ƒè¯•æŠ€å·§

```python
# 1. æ£€æŸ¥æ¢¯åº¦
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: æ¢¯åº¦èŒƒæ•° {param.grad.norm():.6f}")

# 2. ç›‘æ§æŸå¤±
if torch.isnan(loss):
    print("è­¦å‘Š: æŸå¤±ä¸ºNaNï¼Œæ£€æŸ¥å­¦ä¹ ç‡è®¾ç½®")

# 3. å¯è§†åŒ–æ¿€æ´»
def hook_fn(module, input, output):
    print(f"{module.__class__.__name__} è¾“å‡ºå½¢çŠ¶: {output.shape}")

# æ³¨å†Œé’©å­
model.conv1.register_forward_hook(hook_fn)
model.conv2.register_forward_hook(hook_fn)
```

## ğŸ“š å‚è€ƒèµ„æ–™

- **åŸå§‹è®ºæ–‡**: [Gradient-based learning applied to document recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)
- **ä½œè€…**: Y. LeCun, L. Bottou, Y. Bengio, P. Haffner
- **å‘è¡¨å¹´ä»½**: 1998
- **åº”ç”¨é¢†åŸŸ**: æ–‡æ¡£è¯†åˆ«ã€æ‰‹å†™æ•°å­—è¯†åˆ«

## ğŸ“ å­¦ä¹ å»ºè®®

1. **ç†è§£åŸç†**: LeNetæ˜¯ç†è§£CNNçš„æœ€ä½³èµ·ç‚¹
2. **åŠ¨æ‰‹å®è·µ**: åœ¨ä¸åŒæ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹
3. **å¯¹æ¯”åˆ†æ**: ä¸ç°ä»£CNNæ¶æ„å¯¹æ¯”å­¦ä¹ 
4. **å¯è§†åŒ–åˆ†æ**: è§‚å¯Ÿå·ç§¯æ ¸å’Œç‰¹å¾å›¾çš„å˜åŒ–
5. **æ€§èƒ½ä¼˜åŒ–**: å°è¯•ä¸åŒçš„ä¼˜åŒ–æŠ€å·§

LeNet-5è™½ç„¶ç®€å•ï¼Œä½†åŒ…å«äº†å·ç§¯ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ€æƒ³ï¼Œæ˜¯æ·±åº¦å­¦ä¹ å…¥é—¨çš„ç†æƒ³é€‰æ‹©ï¼