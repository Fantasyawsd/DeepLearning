# DeepLearning Models

è¿™æ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ é¢†åŸŸå„ç§ç»å…¸æ¨¡å‹çš„PyTorchå¤ç°é¡¹ç›®ã€‚

*This is a PyTorch implementation project of various classic deep learning models.*

## ğŸ“– Documentation | æ–‡æ¡£

**English Documentation:**
- **[Complete User Guide](docs/USER_GUIDE.md)** - Comprehensive framework tutorial
- **[Getting Started Tutorial](docs/TUTORIAL.md)** - Step-by-step tutorial with examples
- **[MAE Complete Guide](docs/models/MAE_GUIDE.md)** - Detailed MAE model documentation
- **[BERT Complete Guide](docs/models/BERT_GUIDE.md)** - Detailed BERT model documentation  
- **[Swin Transformer Complete Guide](docs/models/SWIN_TRANSFORMER_GUIDE.md)** - Detailed Swin Transformer documentation

**ä¸­æ–‡æ–‡æ¡£:**
- **[ç”¨æˆ·æŒ‡å—](docs/USER_GUIDE.md)** - å®Œæ•´çš„æ¡†æ¶ä½¿ç”¨æ•™ç¨‹
- **[å…¥é—¨æ•™ç¨‹](docs/TUTORIAL.md)** - é€æ­¥æ•™ç¨‹å’Œä»£ç ç¤ºä¾‹
- **[MAE å®Œæ•´æŒ‡å—](docs/models/MAE_GUIDE.md)** - MAEæ¨¡å‹è¯¦ç»†ä½¿ç”¨è¯´æ˜
- **[BERT å®Œæ•´æŒ‡å—](docs/models/BERT_GUIDE.md)** - BERTæ¨¡å‹è¯¦ç»†ä½¿ç”¨è¯´æ˜
- **[Swin Transformer å®Œæ•´æŒ‡å—](docs/models/SWIN_TRANSFORMER_GUIDE.md)** - Swin Transformeræ¨¡å‹è¯¦ç»†ä½¿ç”¨è¯´æ˜

## å·²å®ç°çš„æ¨¡å‹

### 1. MAE (Masked Autoencoder)
- **è®ºæ–‡**: "Masked Autoencoders Are Scalable Vision Learners" (CVPR 2022)
- **ç‰¹ç‚¹**: è‡ªç›‘ç£å­¦ä¹ çš„è§†è§‰Transformerï¼Œé€šè¿‡æ©ç é‡å»ºå­¦ä¹ è§†è§‰è¡¨ç¤º
- **æ–‡ä»¶**: `models/mae.py`
- **é…ç½®**: `configs/mae_config.yaml`

### 2. BERT (Bidirectional Encoder Representations from Transformers)
- **è®ºæ–‡**: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (NAACL 2019)
- **ç‰¹ç‚¹**: åŒå‘Transformerç¼–ç å™¨ï¼Œæ”¯æŒæ©ç è¯­è¨€æ¨¡å‹å’Œåºåˆ—åˆ†ç±»ä»»åŠ¡
- **æ–‡ä»¶**: `models/bert.py`
- **é…ç½®**: `configs/bert_config.yaml`

### 3. Swin Transformer
- **è®ºæ–‡**: "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows" (ICCV 2021)
- **ç‰¹ç‚¹**: åˆ†å±‚è§†è§‰Transformerï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£æœºåˆ¶
- **æ–‡ä»¶**: `models/swin_transformer.py`
- **é…ç½®**: `configs/swin_config.yaml`

## é¡¹ç›®ç»“æ„

```
DeepLearning/
â”œâ”€â”€ models/              # æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py         # åŸºç¡€æ¨¡å‹ç±»
â”‚   â”œâ”€â”€ mae.py          # MAEæ¨¡å‹
â”‚   â”œâ”€â”€ bert.py         # BERTæ¨¡å‹
â”‚   â””â”€â”€ swin_transformer.py  # Swin Transformeræ¨¡å‹
â”œâ”€â”€ utils/              # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py       # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ logger.py       # æ—¥å¿—å·¥å…·
â”‚   â””â”€â”€ metrics.py      # è¯„ä¼°æŒ‡æ ‡
â”œâ”€â”€ configs/            # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ mae_config.yaml
â”‚   â”œâ”€â”€ bert_config.yaml
â”‚   â””â”€â”€ swin_config.yaml
â”œâ”€â”€ examples/           # ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ mae_example.py
â”‚   â”œâ”€â”€ bert_example.py
â”‚   â””â”€â”€ swin_transformer_example.py
â”œâ”€â”€ datasets/           # æ•°æ®é›†å¤„ç†
â”œâ”€â”€ train.py           # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ requirements.txt   # ä¾èµ–åŒ…
â”œâ”€â”€ setup.py          # å®‰è£…è„šæœ¬
â””â”€â”€ readme.md         # é¡¹ç›®è¯´æ˜
```

## å®‰è£…

1. å…‹éš†é¡¹ç›®
```bash
git clone https://github.com/Fantasyawsd/DeepLearning.git
cd DeepLearning
```

2. å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

3. å®‰è£…åŒ…
```bash
pip install -e .
```

## ä½¿ç”¨æ–¹æ³•

### ğŸ“š è¯¦ç»†æ–‡æ¡£

æŸ¥çœ‹å®Œæ•´ä½¿ç”¨æŒ‡å—å’Œæ•™ç¨‹ï¼š
- **[ç”¨æˆ·æŒ‡å—](docs/USER_GUIDE.md)** - å®Œæ•´çš„æ¡†æ¶ä½¿ç”¨æ•™ç¨‹
- **[MAE å®Œæ•´æŒ‡å—](docs/models/MAE_GUIDE.md)** - MAEæ¨¡å‹è¯¦ç»†ä½¿ç”¨è¯´æ˜
- **[BERT å®Œæ•´æŒ‡å—](docs/models/BERT_GUIDE.md)** - BERTæ¨¡å‹è¯¦ç»†ä½¿ç”¨è¯´æ˜
- **[Swin Transformer å®Œæ•´æŒ‡å—](docs/models/SWIN_TRANSFORMER_GUIDE.md)** - Swin Transformeræ¨¡å‹è¯¦ç»†ä½¿ç”¨è¯´æ˜

### 1. å¿«é€Ÿä½“éªŒ

è¿è¡Œç¤ºä¾‹è„šæœ¬ï¼š
```bash
# MAEç¤ºä¾‹
python examples/mae_example.py

# BERTç¤ºä¾‹
python examples/bert_example.py

# Swin Transformerç¤ºä¾‹
python examples/swin_transformer_example.py
```

### 2. è®­ç»ƒæ¨¡å‹

ä½¿ç”¨é…ç½®æ–‡ä»¶è®­ç»ƒæ¨¡å‹ï¼š
```bash
# è®­ç»ƒMAE
python train.py --config configs/mae_config.yaml --output_dir outputs/mae

# è®­ç»ƒBERT
python train.py --config configs/bert_config.yaml --output_dir outputs/bert

# è®­ç»ƒSwin Transformer
python train.py --config configs/swin_config.yaml --output_dir outputs/swin
```

### 3. è‡ªå®šä¹‰ä½¿ç”¨

```python
from models import MAE, BERT, SwinTransformer
from utils import Config

# åŠ è½½é…ç½®
config = Config.from_file('configs/mae_config.yaml')

# åˆ›å»ºæ¨¡å‹
model = MAE(config.to_dict())

# æ¨¡å‹ä¿¡æ¯
model.summary()

# å‰å‘ä¼ æ’­
import torch
x = torch.randn(1, 3, 224, 224)
output = model(x)
```

## æ¨¡å‹ç‰¹æ€§

### é€šç”¨ç‰¹æ€§
- ç»Ÿä¸€çš„åŸºç¡€æ¨¡å‹ç±» (`BaseModel`)
- å®Œæ•´çš„æ£€æŸ¥ç‚¹ä¿å­˜/åŠ è½½æœºåˆ¶
- å‚æ•°å†»ç»“/è§£å†»åŠŸèƒ½
- æ¨¡å‹ä¿¡æ¯ç»Ÿè®¡

### MAEç‰¹æ€§
- å®Œæ•´çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„
- å¯é…ç½®çš„æ©ç æ¯”ä¾‹
- ä½ç½®ç¼–ç æ”¯æŒ
- é‡å»ºæŸå¤±è®¡ç®—

### BERTç‰¹æ€§
- å¤šç§ä»»åŠ¡æ”¯æŒï¼ˆMLMã€åˆ†ç±»ç­‰ï¼‰
- æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–
- çµæ´»çš„è¾“å…¥æ ¼å¼
- é¢„è®­ç»ƒæƒé‡å…¼å®¹

### Swin Transformerç‰¹æ€§
- åˆ†å±‚ç‰¹å¾æå–
- æ»‘åŠ¨çª—å£æ³¨æ„åŠ›
- ç›¸å¯¹ä½ç½®ç¼–ç 
- é«˜æ•ˆçš„è®¡ç®—å¤æ‚åº¦

## ä¾èµ–

- Python >= 3.8
- PyTorch >= 1.12.0
- transformers >= 4.20.0
- einops >= 0.4.1
- timm >= 0.6.7
- å…¶ä»–ä¾èµ–è§ `requirements.txt`

## è´¡çŒ®

æ¬¢è¿æäº¤PRå’ŒIssueï¼è¯·ç¡®ä¿ï¼š
1. ä»£ç ç¬¦åˆé¡¹ç›®è§„èŒƒ
2. æ·»åŠ å¿…è¦çš„æµ‹è¯•
3. æ›´æ–°ç›¸å…³æ–‡æ¡£

## è®¸å¯è¯

MIT License

## è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹è®ºæ–‡çš„ä½œè€…ï¼š
- MAE: He et al., "Masked Autoencoders Are Scalable Vision Learners"
- BERT: Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
- Swin Transformer: Liu et al., "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
